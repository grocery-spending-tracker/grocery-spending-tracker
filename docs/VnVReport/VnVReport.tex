\documentclass[12pt, titlepage]{article}

\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
    colorlinks,
    citecolor=black,
    filecolor=black,
    linkcolor=red,
    urlcolor=blue
}
\usepackage[round]{natbib}
\usepackage{longtable}
\usepackage{amsfonts}

\input{../Comments}
\input{../Common}

\begin{document}

\title{Verification and Validation Report: \progname} 
\author{\authname}
\date{\today}
	
\maketitle

\pagenumbering{roman}

\section{Revision History}

\begin{tabularx}{\textwidth}{p{3cm}p{2cm}X}
\toprule {\bf Date} & {\bf Version} & {\bf Notes}\\
\midrule
Date 1 & 1.0 & Notes\\
Date 2 & 1.1 & Notes\\
\bottomrule
\end{tabularx}

~\newpage

\section{Symbols, Abbreviations and Acronyms}

\renewcommand{\arraystretch}{1.2}
\begin{tabular}{l l} 
  \toprule		
  \textbf{symbol} & \textbf{description}\\
  \midrule 
  T & Test\\
  SRS & Software Requirements Specification\\
  MIS & Module Interface Specification\\
  VnV & Verfication and Validation\\
  \bottomrule
\end{tabular}\\

\wss{symbols, abbreviations or acronyms -- you can reference the SRS tables if needed}

\newpage

\tableofcontents

\listoftables %if appropriate

\listoffigures %if appropriate

\newpage

\pagenumbering{arabic}

This document will primarily focus on the test results of the verification and validation process
for the Grocery Spending Tracker application. In particular, the results of the Functional Requirement System Tests,
Nonfunctional Requirement System Tests, and Unit Tests will be covered. Detailed information regarding
the Functional Requirements and Nonfunctional Requirements can be found in the
\href{https://github.com/r-yeh/grocery-spending-tracker/blob/master/docs/SRS/SRS.pdf}{SRS}. Furthermore,
unit tests will focus on the individual modules of the system which can be found in the
\href{https://github.com/r-yeh/grocery-spending-tracker/blob/master/docs/Design/SoftDetailedDes/MIS.pdf}{MIS}.
Finally, detailed information regarding the overall test plan can be found in the
\href{https://github.com/r-yeh/grocery-spending-tracker/blob/master/docs/VnVPlan/VnVPlan.pdf}{VnV Plan}.\\

In addition to test results, this document will also go over feedback from stakeholders and changes due to testing,
the team's approach for automated testing, traceability between tests and requirements/modules, as well as
overall code coverage.

\section{Functional Requirements Evaluation}

This section will cover the test results from the Functional Requirement System Tests
performed for the verification and validation of the Grocery Spending Tracker application.

...

\section{Nonfunctional Requirements Evaluation}

This section goes over the test results from the Nonfunctional Requirement System Tests as
specified in the VnV Plan.

\subsection{Usability}
		
\subsection{Performance}

\subsection{etc.}
	
% \section{Comparison to Existing Implementation}	

% This section will not be appropriate for every project.

\section{Unit Testing}

This section will go over the test results from the unit tests made as part of the
verification and validation process.

\subsection{Receipt Extraction Module}

The section provides a brief overview of the unit tests performed and their results for the
Receipt Extraction Module.

\begin{longtable}{|l|c|l|c|l|c|}
  \caption{Receipt Extraction Unit Testing Summary} \label{Receipt Extraction Unit Test Summary} \\
  \toprule
  \textbf{Test ID} & \textbf{Result} & \textbf{Test ID} & \textbf{Result} & \textbf{Test ID} & \textbf{Result} \\
  \midrule
  FRT-M3-1 & $\checkmark$ & FRT-M3-2 & $\checkmark$ & FRT-M3-3 & $\checkmark$ \\
  \midrule
  FRT-M3-4 & $\checkmark$ & FRT-M3-5 & $\checkmark$ & FRT-M3-6 & $\checkmark$ \\
  \midrule
  FRT-M3-7 & $\checkmark$ & FRT-M3-8 & $\checkmark$ & FRT-M3-9 & $\checkmark$ \\
  \midrule
  FRT-M3-10 & $\checkmark$ & FRT-M3-11 & $\checkmark$ & FRT-M3-12 & $\checkmark$ \\
  \midrule
  FRT-M3-13 & $\checkmark$ & FRT-M3-14 & $\checkmark$ & FRT-M3-15 & $\checkmark$ \\
  \midrule
  FRT-M3-16 & $\checkmark$ & FRT-M3-17 & $\checkmark$ & FRT-M3-18 & $\checkmark$ \\
  \midrule
  FRT-M3-19 & $\checkmark$ & FRT-M3-20 & $\checkmark$ & FRT-M3-21 & $\checkmark$ \\
  \midrule
  FRT-M3-22 & $\checkmark$ & FRT-M3-23 & $\checkmark$ & FRT-M3-24 & $\checkmark$ \\
  \midrule
  FRT-M3-25 & $\checkmark$ & FRT-M3-26 & $\checkmark$ & ~ & ~ \\
  \bottomrule
\end{longtable}

To summarize, tests FRT-M3-1 to FRT-M3-26 are all frontend tests. These tests focus on the helper functions used
to extract specific information when receiving a chunk of receipt text. FRT-M3-1 to FRT-M3-6 focus on date
extraction with different formats, FRT-M3-7 to FRT-M3-13 test address extraction, FRT-M3-14 to FRT-M3-16 target grocery item extraction,
FRT-M3-17 to FRT-M3-19 focus on subtotal cost extraction, and FRT-M3-20 to FRT-M3-22 tests total cost extraction.
Finally, FRT-M3-23 to FRT-M3-24 and FRT-M3-25 to FRT-M3-26 test updating and JSON conversion for the Item and Grocery Trip
objects respectively. In general, these
tests consist of ideal use cases as well as some edge cases that may appear during use.
For more detailed information regarding the Receipt Extraction Module unit tests, the testing file
can be found here: \href{https://github.com/allanfang1/grocery_spending_tracker_app/blob/main/test/receipt_extraction_test.dart}{receipt extraction
unit tests}.

\subsection{User Analytics Module}

...

\subsection{Users Module}

...

\subsection{Authentication Module}

...

\subsection{Recommendation Engine}

...

\subsection{Classification Engine}

...

\section{Changes Due to Testing}

% \wss{This section should highlight how feedback from the users and from 
% the supervisor (when one exists) shaped the final product.  In particular 
% the feedback from the Rev 0 demo to the supervisor (or to potential users) 
% should be highlighted.}

Prior to formal usability testing, user impressions of the Revision 0 version of the app
generally criticized the usability of the application. Specifically, better communication with
the user in terms of loading. The previous version of the application lacked animated loading
indicators so for forms that had a server request attached to them, they believed the application
to be frozen after submission. This resulted in cases where users would try submitting again producing
a duplicate server request. This has since been resolved by adding various animated loading indicators
and overlays to prevent double clicking a submission and better communicate to users when server requests
were being processed. Some intuitiveness features were also added as a result of user feedback such as
tap focus on the receipt capture page. Additionally, some feedback received from the Revision 0 demo
was addressed in the current version of the application. This included more detailed information
on the Goals page and matching address to a list of known addresses to improve usability/reduce user
input.\\

During formal usability testing of the current application version, much of the feedback had to do with robustness
of the application as opposed to functionality. In particular, the request for submitting a trip
ended up taking close to 10 seconds due to the current Classification Engine implementation. As
a result, users tended to believe that there was something wrong with the application. In response to this,
a backend fix is being worked on to reduce the response time which will be done prior to Revision 1.
Additionally, some users had some issues with the receipt capture, not knowing that they had to get the
receipt to occupy as much of the camera frame as possible for the best results, so an information screen
on this page is being worked on to better help users with receipt capture which will also be finished
prior to Revision 1.

\section{Automated Testing}

On the frontend, all automated tests are carried out using Flutter's built-in testing package.
All tests are located in the "test" folder which is organized by components being tested (found 
\href{https://github.com/allanfang1/grocery_spending_tracker_app/tree/main}{here}). For example,
the \textit{profile\_provider\_test} file executes tests related to the provider for profile functionality.
These tests are executed twice during the development workflow: the first is locally while working on
individual feature branches and the second is on merges/pushes to the "dev" branch of the frontend repository.
The first execution is done manually using \textit{flutter test} in the command line and the second is done
automatically using \textit{Github Actions}.\\

On the backend, automated tests are done using Chai HTTP. Tests are located in the "tests" folder which
is then organized by component and the module being tested 
(found \href{https://github.com/grocery-spending-tracker/grocery-spending-tracker-backend/tree/master/tests}{here}).
As an example, the \textit{userController.analytics.test} file contains unit tests related to the Analytics Module
functionality in the user controller.
These tests are automatically executed during the deployment process of the backend when code is pushed to the
"master" branch of the backend repository. This process is handled by \textit{Github Actions}.
		
\section{Trace to Requirements}
		
\section{Trace to Modules}		

\section{Code Coverage Metrics}

\bibliographystyle{plainnat}
\bibliography{../../refs/References}

\newpage{}
\section{Appendix --- Survey Results}

\noindent \textbf{Question 1:} How easy was it to log into the application?
Responses: \{(not easy) 1, 2, 3, 4, 5 (very easy)\} \\

\subsubsection{Survey questions for \textbf{FRT-FR8-2}.}

\noindent \textbf{Question 1:} How useful would you say that the application recommendations are to you?
Responses: \{(not useful) 1, 2, 3, 4, 5 (very useful)\} \\

\noindent \textbf{Question 2:} How likely are you to act the recommendation provided to you by the application?
Responses: \{(not likely) 1, 2, 3, 4, 5 (very likely)\}

\subsection{Look and Feel Survey Questions}

\begin{enumerate}
  \item Did you feel the colour scheme was cohesive and consistent throughout the application?
  \item On a scale of 1-10, how would you rate the general layouts of each of the pages?
  \item Were there any pages that felt too cluttered or too sparse?
  \item Did the general layouts of the application pages appear consistent throughout navigation?
  \item On a scale of 1-10, how would you rate how features were organized?
  \item Were there any features that seemed out of place relative to the pages they were present on?
  \item Were there any places where you expected loading indicators but there weren't any or vice versa?
  \item Do you have any feedback on the overall look and feel of the application?
\end{enumerate}

\subsection{Usability Survey Questions}

\begin{enumerate}
  \item What parts of the application were easiest for you to understand/learn?
  \item What parts of the application do you feel additional explanation or clarity could be provided?
  \item Was the language and wording used in the application understandable and inoffensive? If not, please
  provide where and how the wording could be improved?
  \item Were there any issues you encountered while using the application?
  \item Did you have any issues with navigating the application? If you made a mistake, were you able to navigate
  back to where you started?
  \item How was your experience with the available accessibility settings? Do you have any feedback or suggestions for
  improvements that could be made.
  \item On a scale of 1-10, how would you rate the usability and intuitiveness of the application?
\end{enumerate}

\newpage{}
\section*{Appendix --- Reflection}

The information in this section will be used to evaluate the team members on the
graduate attribute of Reflection.  Please answer the following question:

\begin{enumerate}
  \item In what ways was the Verification and Validation (VnV) Plan different
  from the activities that were actually conducted for VnV?  If there were
  differences, what changes required the modification in the plan?  Why did
  these changes occur?  Would you be able to anticipate these changes in future
  projects?  If there weren't any differences, how was your team able to clearly
  predict a feasible amount of effort and the right tasks needed to build the
  evidence that demonstrates the required quality?  (It is expected that most
  teams will have had to deviate from their original VnV Plan.)
\end{enumerate}

\end{document}